{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn\n",
    "\n",
    "encoder = tflearn.input_data(shape=[None, 64])\n",
    "encoder = tflearn.fully_connected(encoder, 32)\n",
    "encoder = tflearn.fully_connected(encoder, 16)\n",
    "\n",
    "# Building the decoder\n",
    "decoder = tflearn.fully_connected(encoder, 32)\n",
    "decoder = tflearn.fully_connected(decoder, 64, activation='sigmoid')\n",
    "\n",
    "# Regression, with mean square error\n",
    "net = tflearn.regression(decoder, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='mean_square', metric=None)\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = []\n",
    "\n",
    "for i in range(10):\n",
    "    x_data.append(np.array(range(0, 64 * (i + 1), i + 1)))\n",
    "    \n",
    "x_data = np.array(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: auto_encoder\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 10\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.841s\n",
      "| Adam | epoch: 001 | loss: 0.00000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m46050.74219\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 002 | loss: 46050.74219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m50235.87109\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 003 | loss: 50235.87109 -- iter: 10/10\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m50932.15234\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 004 | loss: 50932.15234 -- iter: 10/10\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m51090.94922\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 005 | loss: 51090.94922 -- iter: 10/10\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m51133.34766\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 006 | loss: 51133.34766 -- iter: 10/10\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m51143.00000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 007 | loss: 51143.00000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m51140.23438\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 008 | loss: 51140.23438 -- iter: 10/10\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m51135.36328\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 009 | loss: 51135.36328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m51118.14062\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 010 | loss: 51118.14062 -- iter: 10/10\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m51104.63281\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 011 | loss: 51104.63281 -- iter: 10/10\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m51080.97266\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 012 | loss: 51080.97266 -- iter: 10/10\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m51059.63281\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 013 | loss: 51059.63281 -- iter: 10/10\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m51051.13281\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 014 | loss: 51051.13281 -- iter: 10/10\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m51033.64844\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 015 | loss: 51033.64844 -- iter: 10/10\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m51021.39844\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 016 | loss: 51021.39844 -- iter: 10/10\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m51012.96875\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 017 | loss: 51012.96875 -- iter: 10/10\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m51007.21484\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 018 | loss: 51007.21484 -- iter: 10/10\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m51003.28125\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 019 | loss: 51003.28125 -- iter: 10/10\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m51000.58203\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 020 | loss: 51000.58203 -- iter: 10/10\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m50998.71875\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 021 | loss: 50998.71875 -- iter: 10/10\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m50997.42188\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 022 | loss: 50997.42188 -- iter: 10/10\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m50996.51953\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 023 | loss: 50996.51953 -- iter: 10/10\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m50995.88672\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 024 | loss: 50995.88672 -- iter: 10/10\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m50995.44141\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 025 | loss: 50995.44141 -- iter: 10/10\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m50995.12891\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 026 | loss: 50995.12891 -- iter: 10/10\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m50994.89844\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 027 | loss: 50994.89844 -- iter: 10/10\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m50994.73438\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 028 | loss: 50994.73438 -- iter: 10/10\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m50994.61328\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 029 | loss: 50994.61328 -- iter: 10/10\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m50994.52344\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 030 | loss: 50994.52344 -- iter: 10/10\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m50994.45703\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 031 | loss: 50994.45703 -- iter: 10/10\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m50994.40625\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 032 | loss: 50994.40625 -- iter: 10/10\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m50994.36719\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 033 | loss: 50994.36719 -- iter: 10/10\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m50994.33984\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 034 | loss: 50994.33984 -- iter: 10/10\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m50994.31641\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 035 | loss: 50994.31641 -- iter: 10/10\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m50994.30078\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 036 | loss: 50994.30078 -- iter: 10/10\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m50994.28906\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 037 | loss: 50994.28906 -- iter: 10/10\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m50994.27734\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 038 | loss: 50994.27734 -- iter: 10/10\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m50994.26953\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 039 | loss: 50994.26953 -- iter: 10/10\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m50994.26562\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 040 | loss: 50994.26562 -- iter: 10/10\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m50994.26172\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 041 | loss: 50994.26172 -- iter: 10/10\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m50994.25781\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 042 | loss: 50994.25781 -- iter: 10/10\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m50994.25391\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 043 | loss: 50994.25391 -- iter: 10/10\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m50994.25000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 044 | loss: 50994.25000 -- iter: 10/10\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m50994.24609\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 045 | loss: 50994.24609 -- iter: 10/10\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 046 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 047 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 048 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 049 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 050 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 051 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 052 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 053 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 054 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 055 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 056 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 057 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m50994.24219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 058 | loss: 50994.24219 -- iter: 10/10\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 059 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 060 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 061 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 062 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 063 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 064 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 065 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 066 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 067 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 068 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 069 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 070 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 071 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 072 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 073 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 074 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 075 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 076 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 077 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 078 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 079 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 080 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 081 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 082 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 083 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 084 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 085 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 086 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 087 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 088 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 089 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 090 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 091 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 092 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 093 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 094 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 095 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 096 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 097 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 098 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 099 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 100 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 101 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 102 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 103 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 104 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 105 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 106 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 107 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 108 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 109 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 110 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 111 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 112 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 113 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 114 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 115 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 116 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 117 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 118 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 119 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 120 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 121 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 122 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 123 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 124 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 125 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 126 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 127 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 128 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 129 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 130 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 131 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 132 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 133 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 134 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 135 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 136 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 137 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 138 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 139 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 140 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 141 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 142 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 143 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 144 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 145 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 146 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 147 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 148 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 149 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 150 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 151 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 152 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 153 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 154 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 155 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 156 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 157 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 158 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 159 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 160 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 161 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 162 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 163 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 164 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 165 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 166 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 167 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 168 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 169 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 170 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 171 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 172 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 173 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 174 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 175 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 176 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 177 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 178 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 179 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 180 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 181 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 182 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 183 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 184 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 185 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 186 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 187 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 188 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 189 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 190 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 191 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 192 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 193 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 194 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 195 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 196 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 197 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 198 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 199 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m50994.23828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 200 | loss: 50994.23828 -- iter: 10/10\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_data, x_data, n_epoch=200, run_id=\"auto_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63],\n",
       "       [  0,   2,   4,   6,   8,  10,  12,  14,  16,  18,  20,  22,  24,\n",
       "         26,  28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,\n",
       "         52,  54,  56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,\n",
       "         78,  80,  82,  84,  86,  88,  90,  92,  94,  96,  98, 100, 102,\n",
       "        104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126],\n",
       "       [  0,   3,   6,   9,  12,  15,  18,  21,  24,  27,  30,  33,  36,\n",
       "         39,  42,  45,  48,  51,  54,  57,  60,  63,  66,  69,  72,  75,\n",
       "         78,  81,  84,  87,  90,  93,  96,  99, 102, 105, 108, 111, 114,\n",
       "        117, 120, 123, 126, 129, 132, 135, 138, 141, 144, 147, 150, 153,\n",
       "        156, 159, 162, 165, 168, 171, 174, 177, 180, 183, 186, 189],\n",
       "       [  0,   4,   8,  12,  16,  20,  24,  28,  32,  36,  40,  44,  48,\n",
       "         52,  56,  60,  64,  68,  72,  76,  80,  84,  88,  92,  96, 100,\n",
       "        104, 108, 112, 116, 120, 124, 128, 132, 136, 140, 144, 148, 152,\n",
       "        156, 160, 164, 168, 172, 176, 180, 184, 188, 192, 196, 200, 204,\n",
       "        208, 212, 216, 220, 224, 228, 232, 236, 240, 244, 248, 252],\n",
       "       [  0,   5,  10,  15,  20,  25,  30,  35,  40,  45,  50,  55,  60,\n",
       "         65,  70,  75,  80,  85,  90,  95, 100, 105, 110, 115, 120, 125,\n",
       "        130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190,\n",
       "        195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255,\n",
       "        260, 265, 270, 275, 280, 285, 290, 295, 300, 305, 310, 315],\n",
       "       [  0,   6,  12,  18,  24,  30,  36,  42,  48,  54,  60,  66,  72,\n",
       "         78,  84,  90,  96, 102, 108, 114, 120, 126, 132, 138, 144, 150,\n",
       "        156, 162, 168, 174, 180, 186, 192, 198, 204, 210, 216, 222, 228,\n",
       "        234, 240, 246, 252, 258, 264, 270, 276, 282, 288, 294, 300, 306,\n",
       "        312, 318, 324, 330, 336, 342, 348, 354, 360, 366, 372, 378],\n",
       "       [  0,   7,  14,  21,  28,  35,  42,  49,  56,  63,  70,  77,  84,\n",
       "         91,  98, 105, 112, 119, 126, 133, 140, 147, 154, 161, 168, 175,\n",
       "        182, 189, 196, 203, 210, 217, 224, 231, 238, 245, 252, 259, 266,\n",
       "        273, 280, 287, 294, 301, 308, 315, 322, 329, 336, 343, 350, 357,\n",
       "        364, 371, 378, 385, 392, 399, 406, 413, 420, 427, 434, 441],\n",
       "       [  0,   8,  16,  24,  32,  40,  48,  56,  64,  72,  80,  88,  96,\n",
       "        104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200,\n",
       "        208, 216, 224, 232, 240, 248, 256, 264, 272, 280, 288, 296, 304,\n",
       "        312, 320, 328, 336, 344, 352, 360, 368, 376, 384, 392, 400, 408,\n",
       "        416, 424, 432, 440, 448, 456, 464, 472, 480, 488, 496, 504],\n",
       "       [  0,   9,  18,  27,  36,  45,  54,  63,  72,  81,  90,  99, 108,\n",
       "        117, 126, 135, 144, 153, 162, 171, 180, 189, 198, 207, 216, 225,\n",
       "        234, 243, 252, 261, 270, 279, 288, 297, 306, 315, 324, 333, 342,\n",
       "        351, 360, 369, 378, 387, 396, 405, 414, 423, 432, 441, 450, 459,\n",
       "        468, 477, 486, 495, 504, 513, 522, 531, 540, 549, 558, 567],\n",
       "       [  0,  10,  20,  30,  40,  50,  60,  70,  80,  90, 100, 110, 120,\n",
       "        130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250,\n",
       "        260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380,\n",
       "        390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510,\n",
       "        520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.3277332566730138e-09,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " [1.8585118818259802e-18,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " [2.6014664353785806e-27,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " [3.641464322248329e-36,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " [0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " [0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " [0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " [0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " [0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0],\n",
       " [0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
