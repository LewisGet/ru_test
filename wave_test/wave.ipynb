{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "y_rate, y_data = [[None] * 2] * 2\n",
    "\n",
    "for i in range(2):\n",
    "    file_path = os.sep.join([os.path.expanduser(\"~lewis\"), \"wave_test\", \"dataset\", str(i + 1) + \".wav\"]);\n",
    "    y_rate[i], y_data[i] = wavfile.read(file_path)\n",
    "    \n",
    "    if y_data[i].shape[0] > max_length:\n",
    "        max_length = y_data[i].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "x_rate, x_data = [[None] * 4] * 2\n",
    "\n",
    "for i in range(4):\n",
    "    file_path = os.sep.join([os.path.expanduser(\"~lewis\"), \"wave_test\", \"dataset\", \"1\", str(i + 1) + \".wav\"]);\n",
    "    x_rate[i], x_data[i] = wavfile.read(file_path)\n",
    "    \n",
    "    if x_data[i].shape[0] > max_length:\n",
    "        max_length = x_data[i].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_format = np.zeros([max_length])\n",
    "\n",
    "for i, data in enumerate(y_data):\n",
    "    tmp_format = np.array(data_format, copy=True)\n",
    "    \n",
    "    # 轉為單聲到，且切齊最常時間檔案\n",
    "    tmp_format[:len(y_data[i])] = [i[0] for i in y_data[i]]\n",
    "    \n",
    "    y_data[i] = tmp_format.reshape(max_length)\n",
    "    \n",
    "y_data = np.array(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_format = np.zeros([max_length])\n",
    "\n",
    "for i, data in enumerate(x_data):\n",
    "    tmp_format = np.array(data_format, copy=True)\n",
    "    \n",
    "    # 轉為單聲到，且切齊最常時間檔案\n",
    "    tmp_format[:len(x_data[i])] = [i[0] for i in x_data[i]]\n",
    "    \n",
    "    x_data[i] = tmp_format.reshape(1, max_length)\n",
    "    \n",
    "x_data = np.array(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 28814)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 28814)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = np.array([y_data[0]] * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 28814)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 28814)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28814"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tflearn.input_data(shape=[None, 1, max_length])\n",
    "net = tflearn.lstm(net, 128, return_seq=True)\n",
    "net = tflearn.lstm(net, 128)\n",
    "net = tflearn.fully_connected(net, max_length, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam',\n",
    "                         loss='categorical_crossentropy', name=\"output1\")\n",
    "model = tflearn.DNN(net, tensorboard_verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/lewis/wave_test/wave_default_lstm\n"
     ]
    }
   ],
   "source": [
    "model.load(\"wave_default_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: KV4L2Q\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/LSTM.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 3\n",
      "Validation samples: 1\n",
      "--\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.981s\n",
      "| Adam | epoch: 001 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05273 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.596s\n",
      "| Adam | epoch: 002 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.643s\n",
      "| Adam | epoch: 003 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 004 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.660s\n",
      "| Adam | epoch: 005 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.651s\n",
      "| Adam | epoch: 006 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.657s\n",
      "| Adam | epoch: 007 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.564s\n",
      "| Adam | epoch: 008 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05273 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 009 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 010 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 011 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 012 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.657s\n",
      "| Adam | epoch: 013 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.568s\n",
      "| Adam | epoch: 014 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05176 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.632s\n",
      "| Adam | epoch: 015 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05176 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.620s\n",
      "| Adam | epoch: 016 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.545s\n",
      "| Adam | epoch: 017 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 018 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.644s\n",
      "| Adam | epoch: 019 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.596s\n",
      "| Adam | epoch: 020 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05176 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 021 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.663s\n",
      "| Adam | epoch: 022 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.570s\n",
      "| Adam | epoch: 023 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.611s\n",
      "| Adam | epoch: 024 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.671s\n",
      "| Adam | epoch: 025 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.598s\n",
      "| Adam | epoch: 026 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 027 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.635s\n",
      "| Adam | epoch: 028 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.649s\n",
      "| Adam | epoch: 029 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 030 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 031 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 032 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 033 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 034 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.630s\n",
      "| Adam | epoch: 035 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.564s\n",
      "| Adam | epoch: 036 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.620s\n",
      "| Adam | epoch: 037 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.605s\n",
      "| Adam | epoch: 038 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 039 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 040 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 041 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 042 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 043 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.584s\n",
      "| Adam | epoch: 044 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 045 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 046 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.618s\n",
      "| Adam | epoch: 047 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 048 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 049 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 050 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.567s\n",
      "| Adam | epoch: 051 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 052 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 053 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 054 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.628s\n",
      "| Adam | epoch: 055 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 056 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 057 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 058 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.574s\n",
      "| Adam | epoch: 059 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.612s\n",
      "| Adam | epoch: 060 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.597s\n",
      "| Adam | epoch: 061 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 062 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.654s\n",
      "| Adam | epoch: 063 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 064 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 065 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 066 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 067 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04785 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 068 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 069 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 070 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.569s\n",
      "| Adam | epoch: 071 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 072 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 073 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 074 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.665s\n",
      "| Adam | epoch: 075 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.666s\n",
      "| Adam | epoch: 076 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 077 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 078 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.640s\n",
      "| Adam | epoch: 079 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 080 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 081 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.599s\n",
      "| Adam | epoch: 082 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.622s\n",
      "| Adam | epoch: 083 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.577s\n",
      "| Adam | epoch: 084 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.607s\n",
      "| Adam | epoch: 085 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.572s\n",
      "| Adam | epoch: 086 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 087 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.656s\n",
      "| Adam | epoch: 088 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.589s\n",
      "| Adam | epoch: 089 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 090 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 091 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 092 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 093 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 094 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.676s\n",
      "| Adam | epoch: 095 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 096 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.563s\n",
      "| Adam | epoch: 097 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 098 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.644s\n",
      "| Adam | epoch: 099 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 100 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.649s\n",
      "| Adam | epoch: 101 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 102 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 103 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 104 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.599s\n",
      "| Adam | epoch: 105 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 106 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 107 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.637s\n",
      "| Adam | epoch: 108 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.642s\n",
      "| Adam | epoch: 109 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 110 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.563s\n",
      "| Adam | epoch: 111 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.563s\n",
      "| Adam | epoch: 112 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.586s\n",
      "| Adam | epoch: 113 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 114 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 115 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.573s\n",
      "| Adam | epoch: 116 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 117 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 118 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 119 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 120 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.578s\n",
      "| Adam | epoch: 121 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.608s\n",
      "| Adam | epoch: 122 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.683s\n",
      "| Adam | epoch: 123 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 124 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.610s\n",
      "| Adam | epoch: 125 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 126 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.649s\n",
      "| Adam | epoch: 127 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.637s\n",
      "| Adam | epoch: 128 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.560s\n",
      "| Adam | epoch: 129 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.578s\n",
      "| Adam | epoch: 130 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.544s\n",
      "| Adam | epoch: 131 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 132 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.600s\n",
      "| Adam | epoch: 133 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 134 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 135 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.585s\n",
      "| Adam | epoch: 136 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 137 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.625s\n",
      "| Adam | epoch: 138 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 139 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 140 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 141 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.597s\n",
      "| Adam | epoch: 142 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 143 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 144 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 145 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 146 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 147 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 148 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.569s\n",
      "| Adam | epoch: 149 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.572s\n",
      "| Adam | epoch: 150 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.665s\n",
      "| Adam | epoch: 151 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.563s\n",
      "| Adam | epoch: 152 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 153 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 154 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.571s\n",
      "| Adam | epoch: 155 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.626s\n",
      "| Adam | epoch: 156 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 157 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.654s\n",
      "| Adam | epoch: 158 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.644s\n",
      "| Adam | epoch: 159 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.634s\n",
      "| Adam | epoch: 160 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.667s\n",
      "| Adam | epoch: 161 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 162 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.638s\n",
      "| Adam | epoch: 163 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.545s\n",
      "| Adam | epoch: 164 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 165 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.566s\n",
      "| Adam | epoch: 166 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.567s\n",
      "| Adam | epoch: 167 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 168 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.567s\n",
      "| Adam | epoch: 169 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 170 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.649s\n",
      "| Adam | epoch: 171 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 172 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 173 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.623s\n",
      "| Adam | epoch: 174 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 175 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 176 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 177 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.564s\n",
      "| Adam | epoch: 178 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 179 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.679s\n",
      "| Adam | epoch: 180 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.590s\n",
      "| Adam | epoch: 181 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.595s\n",
      "| Adam | epoch: 182 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 183 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 184 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 185 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.560s\n",
      "| Adam | epoch: 186 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 187 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.578s\n",
      "| Adam | epoch: 188 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 189 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.650s\n",
      "| Adam | epoch: 190 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 191 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 192 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 193 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.659s\n",
      "| Adam | epoch: 194 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.625s\n",
      "| Adam | epoch: 195 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 196 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 197 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 198 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.686s\n",
      "| Adam | epoch: 199 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.611s\n",
      "| Adam | epoch: 200 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.545s\n",
      "| Adam | epoch: 201 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 202 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.586s\n",
      "| Adam | epoch: 203 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.642s\n",
      "| Adam | epoch: 204 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.586s\n",
      "| Adam | epoch: 205 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.672s\n",
      "| Adam | epoch: 206 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.580s\n",
      "| Adam | epoch: 207 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 208 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.639s\n",
      "| Adam | epoch: 209 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 210 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.621s\n",
      "| Adam | epoch: 211 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 212 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.667s\n",
      "| Adam | epoch: 213 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.683s\n",
      "| Adam | epoch: 214 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 215 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.650s\n",
      "| Adam | epoch: 216 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 217 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 218 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.662s\n",
      "| Adam | epoch: 219 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.635s\n",
      "| Adam | epoch: 220 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 221 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 222 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 223 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 224 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 225 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.578s\n",
      "| Adam | epoch: 226 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.600s\n",
      "| Adam | epoch: 227 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 228 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.584s\n",
      "| Adam | epoch: 229 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.641s\n",
      "| Adam | epoch: 230 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 231 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 232 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.664s\n",
      "| Adam | epoch: 233 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.634s\n",
      "| Adam | epoch: 234 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.639s\n",
      "| Adam | epoch: 235 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 236 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 237 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.598s\n",
      "| Adam | epoch: 238 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 239 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.652s\n",
      "| Adam | epoch: 240 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 241 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.636s\n",
      "| Adam | epoch: 242 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.599s\n",
      "| Adam | epoch: 243 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 244 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 245 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.607s\n",
      "| Adam | epoch: 246 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 247 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.632s\n",
      "| Adam | epoch: 248 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.670s\n",
      "| Adam | epoch: 249 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 250 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 251 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.630s\n",
      "| Adam | epoch: 252 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.642s\n",
      "| Adam | epoch: 253 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 254 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.646s\n",
      "| Adam | epoch: 255 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 256 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.649s\n",
      "| Adam | epoch: 257 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.605s\n",
      "| Adam | epoch: 258 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 259 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.662s\n",
      "| Adam | epoch: 260 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.677s\n",
      "| Adam | epoch: 261 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.686s\n",
      "| Adam | epoch: 262 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.589s\n",
      "| Adam | epoch: 263 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 264 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.566s\n",
      "| Adam | epoch: 265 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.600s\n",
      "| Adam | epoch: 266 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 267 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.624s\n",
      "| Adam | epoch: 268 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 269 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 270 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 271 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 272 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.599s\n",
      "| Adam | epoch: 273 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.678s\n",
      "| Adam | epoch: 274 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 275 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 276 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 277 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 278 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.603s\n",
      "| Adam | epoch: 279 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 280 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.615s\n",
      "| Adam | epoch: 281 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.646s\n",
      "| Adam | epoch: 282 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.600s\n",
      "| Adam | epoch: 283 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.601s\n",
      "| Adam | epoch: 284 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.647s\n",
      "| Adam | epoch: 285 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 286 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 287 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 288 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 289 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.617s\n",
      "| Adam | epoch: 290 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.658s\n",
      "| Adam | epoch: 291 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.644s\n",
      "| Adam | epoch: 292 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 293 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 294 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 295 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 296 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 297 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.564s\n",
      "| Adam | epoch: 298 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.666s\n",
      "| Adam | epoch: 299 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 300 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.651s\n",
      "| Adam | epoch: 301 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.661s\n",
      "| Adam | epoch: 302 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 303 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.545s\n",
      "| Adam | epoch: 304 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 305 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m-9515.05176\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 306 | loss: -9515.05176 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.669s\n",
      "| Adam | epoch: 307 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 308 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 309 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 310 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 311 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.645s\n",
      "| Adam | epoch: 312 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.570s\n",
      "| Adam | epoch: 313 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.650s\n",
      "| Adam | epoch: 314 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.545s\n",
      "| Adam | epoch: 315 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 316 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 317 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 318 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 319 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 320 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 321 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.588s\n",
      "| Adam | epoch: 322 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 323 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 324 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.618s\n",
      "| Adam | epoch: 325 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.646s\n",
      "| Adam | epoch: 326 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 327 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 328 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.567s\n",
      "| Adam | epoch: 329 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.591s\n",
      "| Adam | epoch: 330 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 331 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.647s\n",
      "| Adam | epoch: 332 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.610s\n",
      "| Adam | epoch: 333 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.666s\n",
      "| Adam | epoch: 334 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 335 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.563s\n",
      "| Adam | epoch: 336 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.660s\n",
      "| Adam | epoch: 337 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.645s\n",
      "| Adam | epoch: 338 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 339 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 340 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.567s\n",
      "| Adam | epoch: 341 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.580s\n",
      "| Adam | epoch: 342 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 343 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 344 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.663s\n",
      "| Adam | epoch: 345 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.569s\n",
      "| Adam | epoch: 346 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.659s\n",
      "| Adam | epoch: 347 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 348 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 349 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.567s\n",
      "| Adam | epoch: 350 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.616s\n",
      "| Adam | epoch: 351 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 352 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 353 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 354 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.621s\n",
      "| Adam | epoch: 355 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 356 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.626s\n",
      "| Adam | epoch: 357 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.654s\n",
      "| Adam | epoch: 358 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.576s\n",
      "| Adam | epoch: 359 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.640s\n",
      "| Adam | epoch: 360 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 361 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 362 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 363 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 364 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.630s\n",
      "| Adam | epoch: 365 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.569s\n",
      "| Adam | epoch: 366 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.629s\n",
      "| Adam | epoch: 367 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 368 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.673s\n",
      "| Adam | epoch: 369 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 370 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 371 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 372 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.628s\n",
      "| Adam | epoch: 373 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.579s\n",
      "| Adam | epoch: 374 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.623s\n",
      "| Adam | epoch: 375 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 376 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 377 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.621s\n",
      "| Adam | epoch: 378 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.619s\n",
      "| Adam | epoch: 379 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 380 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.642s\n",
      "| Adam | epoch: 381 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 382 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 383 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.643s\n",
      "| Adam | epoch: 384 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.629s\n",
      "| Adam | epoch: 385 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 386 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.613s\n",
      "| Adam | epoch: 387 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.675s\n",
      "| Adam | epoch: 388 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 389 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.639s\n",
      "| Adam | epoch: 390 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 391 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 392 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 393 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.635s\n",
      "| Adam | epoch: 394 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 395 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.657s\n",
      "| Adam | epoch: 396 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 397 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 398 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 399 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 400 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.572s\n",
      "| Adam | epoch: 401 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.667s\n",
      "| Adam | epoch: 402 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.633s\n",
      "| Adam | epoch: 403 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 404 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.595s\n",
      "| Adam | epoch: 405 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.578s\n",
      "| Adam | epoch: 406 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.593s\n",
      "| Adam | epoch: 407 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 408 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 409 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.621s\n",
      "| Adam | epoch: 410 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.603s\n",
      "| Adam | epoch: 411 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.631s\n",
      "| Adam | epoch: 412 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 413 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 414 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 415 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.564s\n",
      "| Adam | epoch: 416 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.620s\n",
      "| Adam | epoch: 417 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 418 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.605s\n",
      "| Adam | epoch: 419 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 420 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 421 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 422 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.654s\n",
      "| Adam | epoch: 423 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 424 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.575s\n",
      "| Adam | epoch: 425 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.589s\n",
      "| Adam | epoch: 426 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.572s\n",
      "| Adam | epoch: 427 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.563s\n",
      "| Adam | epoch: 428 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.574s\n",
      "| Adam | epoch: 429 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.686s\n",
      "| Adam | epoch: 430 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.582s\n",
      "| Adam | epoch: 431 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 432 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 433 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.565s\n",
      "| Adam | epoch: 434 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.642s\n",
      "| Adam | epoch: 435 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 436 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 437 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 438 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 439 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.564s\n",
      "| Adam | epoch: 440 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.635s\n",
      "| Adam | epoch: 441 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.678s\n",
      "| Adam | epoch: 442 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.638s\n",
      "| Adam | epoch: 443 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 444 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.545s\n",
      "| Adam | epoch: 445 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.560s\n",
      "| Adam | epoch: 446 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.622s\n",
      "| Adam | epoch: 447 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.567s\n",
      "| Adam | epoch: 448 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.624s\n",
      "| Adam | epoch: 449 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 450 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 451 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.613s\n",
      "| Adam | epoch: 452 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 453 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 454 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.643s\n",
      "| Adam | epoch: 455 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 456 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 457 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 458 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 459 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.619s\n",
      "| Adam | epoch: 460 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.642s\n",
      "| Adam | epoch: 461 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 462 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 463 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.649s\n",
      "| Adam | epoch: 464 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 465 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 466 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.672s\n",
      "| Adam | epoch: 467 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.672s\n",
      "| Adam | epoch: 468 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.617s\n",
      "| Adam | epoch: 469 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 470 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 471 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.664s\n",
      "| Adam | epoch: 472 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 473 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.685s\n",
      "| Adam | epoch: 474 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.666s\n",
      "| Adam | epoch: 475 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.653s\n",
      "| Adam | epoch: 476 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 477 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.684s\n",
      "| Adam | epoch: 478 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.660s\n",
      "| Adam | epoch: 479 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.606s\n",
      "| Adam | epoch: 480 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 481 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 482 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 483 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.590s\n",
      "| Adam | epoch: 484 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 485 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 486 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 487 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 488 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.673s\n",
      "| Adam | epoch: 489 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.589s\n",
      "| Adam | epoch: 490 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 491 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 492 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.565s\n",
      "| Adam | epoch: 493 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 494 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 495 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.606s\n",
      "| Adam | epoch: 496 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.632s\n",
      "| Adam | epoch: 497 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.661s\n",
      "| Adam | epoch: 498 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.650s\n",
      "| Adam | epoch: 499 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 500 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 501 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 502 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.545s\n",
      "| Adam | epoch: 503 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 504 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 505 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 506 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 507 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 508 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 509 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 510 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 511 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 512 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 513 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 514 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 515 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 516 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 517 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.658s\n",
      "| Adam | epoch: 518 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 519 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 520 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 521 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.677s\n",
      "| Adam | epoch: 522 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 523 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.619s\n",
      "| Adam | epoch: 524 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 525 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 526 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 527 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 528 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 529 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.613s\n",
      "| Adam | epoch: 530 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.647s\n",
      "| Adam | epoch: 531 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.656s\n",
      "| Adam | epoch: 532 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 533 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 534 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 535 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.629s\n",
      "| Adam | epoch: 536 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.667s\n",
      "| Adam | epoch: 537 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 538 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 539 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.654s\n",
      "| Adam | epoch: 540 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 541 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.565s\n",
      "| Adam | epoch: 542 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.629s\n",
      "| Adam | epoch: 543 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.584s\n",
      "| Adam | epoch: 544 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.614s\n",
      "| Adam | epoch: 545 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 546 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 547 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 548 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.581s\n",
      "| Adam | epoch: 549 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.560s\n",
      "| Adam | epoch: 550 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 551 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 552 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.651s\n",
      "| Adam | epoch: 553 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.598s\n",
      "| Adam | epoch: 554 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 555 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 556 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.579s\n",
      "| Adam | epoch: 557 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 558 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 559 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.585s\n",
      "| Adam | epoch: 560 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 561 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 562 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.564s\n",
      "| Adam | epoch: 563 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 564 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 565 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 566 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.608s\n",
      "| Adam | epoch: 567 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.560s\n",
      "| Adam | epoch: 568 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 569 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 570 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 571 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 572 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 573 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.560s\n",
      "| Adam | epoch: 574 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 575 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.566s\n",
      "| Adam | epoch: 576 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 577 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 578 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 579 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 580 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1181  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.605s\n",
      "| Adam | epoch: 581 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1182  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 582 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1183  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.674s\n",
      "| Adam | epoch: 583 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1184  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 584 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1185  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.661s\n",
      "| Adam | epoch: 585 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1186  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 586 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1187  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 587 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1188  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.679s\n",
      "| Adam | epoch: 588 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1189  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.629s\n",
      "| Adam | epoch: 589 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1190  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 590 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1191  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 591 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1192  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.615s\n",
      "| Adam | epoch: 592 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1193  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 593 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1194  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.580s\n",
      "| Adam | epoch: 594 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1195  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 595 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1196  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 596 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1197  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.611s\n",
      "| Adam | epoch: 597 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1198  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.568s\n",
      "| Adam | epoch: 598 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1199  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 599 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1200  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 600 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1201  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 601 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1202  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 602 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1203  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 603 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1204  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.572s\n",
      "| Adam | epoch: 604 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1205  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 605 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1206  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.605s\n",
      "| Adam | epoch: 606 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1207  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.564s\n",
      "| Adam | epoch: 607 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1208  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.581s\n",
      "| Adam | epoch: 608 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1209  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.565s\n",
      "| Adam | epoch: 609 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1210  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 610 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1211  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 611 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1212  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.632s\n",
      "| Adam | epoch: 612 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1213  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 613 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1214  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.641s\n",
      "| Adam | epoch: 614 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1215  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.622s\n",
      "| Adam | epoch: 615 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1216  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.665s\n",
      "| Adam | epoch: 616 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1217  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.680s\n",
      "| Adam | epoch: 617 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1218  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 618 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1219  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.659s\n",
      "| Adam | epoch: 619 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1220  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.573s\n",
      "| Adam | epoch: 620 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1221  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.611s\n",
      "| Adam | epoch: 621 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1222  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 622 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1223  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.664s\n",
      "| Adam | epoch: 623 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1224  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.611s\n",
      "| Adam | epoch: 624 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1225  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 625 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1226  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.638s\n",
      "| Adam | epoch: 626 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1227  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 627 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1228  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.646s\n",
      "| Adam | epoch: 628 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1229  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.564s\n",
      "| Adam | epoch: 629 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1230  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 630 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1231  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 631 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1232  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 632 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1233  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.619s\n",
      "| Adam | epoch: 633 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1234  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 634 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1235  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.560s\n",
      "| Adam | epoch: 635 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1236  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.594s\n",
      "| Adam | epoch: 636 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1237  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.645s\n",
      "| Adam | epoch: 637 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1238  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.656s\n",
      "| Adam | epoch: 638 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1239  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 639 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1240  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 640 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1241  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 641 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1242  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.609s\n",
      "| Adam | epoch: 642 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1243  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.560s\n",
      "| Adam | epoch: 643 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1244  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.681s\n",
      "| Adam | epoch: 644 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1245  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.578s\n",
      "| Adam | epoch: 645 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1246  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.571s\n",
      "| Adam | epoch: 646 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1247  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 647 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1248  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.617s\n",
      "| Adam | epoch: 648 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 649 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 650 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1251  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.655s\n",
      "| Adam | epoch: 651 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1252  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 652 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1253  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.656s\n",
      "| Adam | epoch: 653 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1254  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 654 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1255  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 655 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1256  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.565s\n",
      "| Adam | epoch: 656 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1257  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 657 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1258  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.652s\n",
      "| Adam | epoch: 658 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1259  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.569s\n",
      "| Adam | epoch: 659 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1260  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 660 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1261  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 661 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1262  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 662 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1263  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 663 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.05078 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1264  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.640s\n",
      "| Adam | epoch: 664 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1265  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.560s\n",
      "| Adam | epoch: 665 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1266  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.648s\n",
      "| Adam | epoch: 666 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1267  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 667 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1268  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.671s\n",
      "| Adam | epoch: 668 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1269  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.586s\n",
      "| Adam | epoch: 669 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1270  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 670 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1271  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 671 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1272  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.560s\n",
      "| Adam | epoch: 672 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1273  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 673 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1274  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 674 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1275  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.567s\n",
      "| Adam | epoch: 675 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1276  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.575s\n",
      "| Adam | epoch: 676 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1277  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 677 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1278  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 678 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1279  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 679 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1280  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 680 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1281  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.570s\n",
      "| Adam | epoch: 681 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1282  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.638s\n",
      "| Adam | epoch: 682 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1283  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.617s\n",
      "| Adam | epoch: 683 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1284  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 684 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1285  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.565s\n",
      "| Adam | epoch: 685 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1286  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 686 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1287  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 687 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1288  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 688 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1289  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 689 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1290  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 690 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1291  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 691 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1292  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.639s\n",
      "| Adam | epoch: 692 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1293  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 693 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1294  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.618s\n",
      "| Adam | epoch: 694 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1295  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 695 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1296  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.642s\n",
      "| Adam | epoch: 696 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1297  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.615s\n",
      "| Adam | epoch: 697 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1298  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 698 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1299  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.646s\n",
      "| Adam | epoch: 699 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04785 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1300  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.675s\n",
      "| Adam | epoch: 700 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1301  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.651s\n",
      "| Adam | epoch: 701 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1302  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 702 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1303  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 703 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1304  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.574s\n",
      "| Adam | epoch: 704 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1305  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.581s\n",
      "| Adam | epoch: 705 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04785 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1306  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 706 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1307  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 707 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1308  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 708 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1309  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.571s\n",
      "| Adam | epoch: 709 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1310  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.683s\n",
      "| Adam | epoch: 710 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1311  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.651s\n",
      "| Adam | epoch: 711 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1312  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.675s\n",
      "| Adam | epoch: 712 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1313  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 713 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1314  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.545s\n",
      "| Adam | epoch: 714 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1315  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.563s\n",
      "| Adam | epoch: 715 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1316  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.656s\n",
      "| Adam | epoch: 716 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1317  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 717 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1318  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 718 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1319  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 719 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1320  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 720 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1321  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.613s\n",
      "| Adam | epoch: 721 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1322  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.653s\n",
      "| Adam | epoch: 722 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1323  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 723 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1324  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 724 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1325  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 725 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1326  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.573s\n",
      "| Adam | epoch: 726 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1327  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.600s\n",
      "| Adam | epoch: 727 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1328  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.663s\n",
      "| Adam | epoch: 728 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1329  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.644s\n",
      "| Adam | epoch: 729 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1330  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 730 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1331  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 731 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1332  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 732 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1333  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 733 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1334  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 734 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1335  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.597s\n",
      "| Adam | epoch: 735 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1336  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.651s\n",
      "| Adam | epoch: 736 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1337  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.610s\n",
      "| Adam | epoch: 737 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1338  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 738 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1339  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.623s\n",
      "| Adam | epoch: 739 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1340  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.583s\n",
      "| Adam | epoch: 740 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1341  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.607s\n",
      "| Adam | epoch: 741 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1342  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.594s\n",
      "| Adam | epoch: 742 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1343  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 743 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1344  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 744 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1345  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.569s\n",
      "| Adam | epoch: 745 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1346  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 746 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1347  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.645s\n",
      "| Adam | epoch: 747 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1348  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 748 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1349  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.638s\n",
      "| Adam | epoch: 749 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1350  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 750 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1351  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 751 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1352  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 752 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1353  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 753 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1354  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.565s\n",
      "| Adam | epoch: 754 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1355  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.618s\n",
      "| Adam | epoch: 755 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1356  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.567s\n",
      "| Adam | epoch: 756 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1357  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 757 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1358  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.600s\n",
      "| Adam | epoch: 758 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1359  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.635s\n",
      "| Adam | epoch: 759 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1360  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.677s\n",
      "| Adam | epoch: 760 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1361  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 761 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1362  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 762 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1363  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.565s\n",
      "| Adam | epoch: 763 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1364  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.651s\n",
      "| Adam | epoch: 764 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1365  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.663s\n",
      "| Adam | epoch: 765 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1366  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.615s\n",
      "| Adam | epoch: 766 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1367  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 767 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1368  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 768 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1369  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 769 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1370  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.599s\n",
      "| Adam | epoch: 770 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1371  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 771 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1372  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 772 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1373  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.570s\n",
      "| Adam | epoch: 773 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1374  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 774 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1375  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 775 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1376  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.565s\n",
      "| Adam | epoch: 776 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1377  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.648s\n",
      "| Adam | epoch: 777 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1378  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.627s\n",
      "| Adam | epoch: 778 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1379  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.633s\n",
      "| Adam | epoch: 779 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1380  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.565s\n",
      "| Adam | epoch: 780 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1381  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 781 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1382  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 782 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1383  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.665s\n",
      "| Adam | epoch: 783 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1384  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.610s\n",
      "| Adam | epoch: 784 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04785 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1385  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.572s\n",
      "| Adam | epoch: 785 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1386  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 786 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1387  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.565s\n",
      "| Adam | epoch: 787 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1388  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.679s\n",
      "| Adam | epoch: 788 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1389  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.582s\n",
      "| Adam | epoch: 789 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1390  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.633s\n",
      "| Adam | epoch: 790 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1391  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.569s\n",
      "| Adam | epoch: 791 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1392  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.580s\n",
      "| Adam | epoch: 792 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1393  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 793 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1394  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.563s\n",
      "| Adam | epoch: 794 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1395  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.583s\n",
      "| Adam | epoch: 795 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1396  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 796 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1397  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.545s\n",
      "| Adam | epoch: 797 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1398  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 798 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1399  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 799 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1400  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.635s\n",
      "| Adam | epoch: 800 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1401  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.568s\n",
      "| Adam | epoch: 801 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1402  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.594s\n",
      "| Adam | epoch: 802 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1403  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 803 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1404  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.629s\n",
      "| Adam | epoch: 804 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1405  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.677s\n",
      "| Adam | epoch: 805 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1406  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.593s\n",
      "| Adam | epoch: 806 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1407  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 807 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1408  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.667s\n",
      "| Adam | epoch: 808 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1409  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 809 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 810 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1411  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.603s\n",
      "| Adam | epoch: 811 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1412  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.640s\n",
      "| Adam | epoch: 812 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1413  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 813 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1414  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 814 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1415  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 815 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1416  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 816 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1417  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.586s\n",
      "| Adam | epoch: 817 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1418  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.656s\n",
      "| Adam | epoch: 818 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1419  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.568s\n",
      "| Adam | epoch: 819 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1420  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 820 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1421  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 821 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1422  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.577s\n",
      "| Adam | epoch: 822 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1423  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.572s\n",
      "| Adam | epoch: 823 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1424  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 824 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1425  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.609s\n",
      "| Adam | epoch: 825 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1426  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.617s\n",
      "| Adam | epoch: 826 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1427  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.545s\n",
      "| Adam | epoch: 827 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1428  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.653s\n",
      "| Adam | epoch: 828 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1429  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 829 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1430  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 830 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1431  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 831 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1432  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 832 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1433  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 833 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1434  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 834 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1435  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 835 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1436  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.677s\n",
      "| Adam | epoch: 836 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1437  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 837 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1438  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 838 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1439  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.571s\n",
      "| Adam | epoch: 839 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1440  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.637s\n",
      "| Adam | epoch: 840 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1441  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 841 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1442  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 842 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1443  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.646s\n",
      "| Adam | epoch: 843 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1444  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.564s\n",
      "| Adam | epoch: 844 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1445  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 845 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1446  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.614s\n",
      "| Adam | epoch: 846 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1447  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.560s\n",
      "| Adam | epoch: 847 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1448  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 848 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1449  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.587s\n",
      "| Adam | epoch: 849 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1450  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 850 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1451  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.563s\n",
      "| Adam | epoch: 851 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1452  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 852 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1453  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 853 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1454  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 854 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1455  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.670s\n",
      "| Adam | epoch: 855 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1456  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.572s\n",
      "| Adam | epoch: 856 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1457  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 857 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1458  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 858 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1459  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.622s\n",
      "| Adam | epoch: 859 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1460  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.574s\n",
      "| Adam | epoch: 860 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1461  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 861 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1462  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 862 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1463  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.612s\n",
      "| Adam | epoch: 863 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1464  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 864 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1465  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.681s\n",
      "| Adam | epoch: 865 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1466  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.646s\n",
      "| Adam | epoch: 866 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1467  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 867 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1468  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.639s\n",
      "| Adam | epoch: 868 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1469  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.653s\n",
      "| Adam | epoch: 869 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1470  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.654s\n",
      "| Adam | epoch: 870 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1471  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.569s\n",
      "| Adam | epoch: 871 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1472  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 872 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1473  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 873 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1474  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.567s\n",
      "| Adam | epoch: 874 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1475  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 875 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1476  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 876 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1477  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 877 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1478  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 878 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1479  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 879 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1480  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.565s\n",
      "| Adam | epoch: 880 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1481  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 881 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1482  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.580s\n",
      "| Adam | epoch: 882 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1483  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.634s\n",
      "| Adam | epoch: 883 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1484  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.628s\n",
      "| Adam | epoch: 884 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1485  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.651s\n",
      "| Adam | epoch: 885 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1486  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 886 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1487  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.581s\n",
      "| Adam | epoch: 887 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1488  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.563s\n",
      "| Adam | epoch: 888 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1489  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.571s\n",
      "| Adam | epoch: 889 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1490  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.560s\n",
      "| Adam | epoch: 890 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1491  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 891 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1492  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.641s\n",
      "| Adam | epoch: 892 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1493  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 893 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1494  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 894 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1495  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 895 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1496  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 896 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1497  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 897 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1498  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.674s\n",
      "| Adam | epoch: 898 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1499  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.569s\n",
      "| Adam | epoch: 899 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1500  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 900 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1501  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.653s\n",
      "| Adam | epoch: 901 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1502  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 902 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1503  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.666s\n",
      "| Adam | epoch: 903 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1504  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 904 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1505  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.619s\n",
      "| Adam | epoch: 905 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1506  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 906 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1507  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.661s\n",
      "| Adam | epoch: 907 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1508  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.564s\n",
      "| Adam | epoch: 908 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1509  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.616s\n",
      "| Adam | epoch: 909 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1510  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 910 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1511  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.651s\n",
      "| Adam | epoch: 911 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1512  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.653s\n",
      "| Adam | epoch: 912 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1513  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 913 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1514  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 914 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1515  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.568s\n",
      "| Adam | epoch: 915 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1516  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 916 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1517  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.576s\n",
      "| Adam | epoch: 917 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1518  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.570s\n",
      "| Adam | epoch: 918 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1519  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 919 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1520  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.572s\n",
      "| Adam | epoch: 920 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1521  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.571s\n",
      "| Adam | epoch: 921 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1522  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.622s\n",
      "| Adam | epoch: 922 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1523  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.628s\n",
      "| Adam | epoch: 923 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1524  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.628s\n",
      "| Adam | epoch: 924 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04785 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1525  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.550s\n",
      "| Adam | epoch: 925 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1526  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.623s\n",
      "| Adam | epoch: 926 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1527  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.661s\n",
      "| Adam | epoch: 927 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1528  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.572s\n",
      "| Adam | epoch: 928 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1529  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.578s\n",
      "| Adam | epoch: 929 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1530  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.617s\n",
      "| Adam | epoch: 930 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1531  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 931 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1532  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.591s\n",
      "| Adam | epoch: 932 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1533  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.583s\n",
      "| Adam | epoch: 933 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1534  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.632s\n",
      "| Adam | epoch: 934 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1535  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.601s\n",
      "| Adam | epoch: 935 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1536  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 936 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1537  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.678s\n",
      "| Adam | epoch: 937 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1538  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 938 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04785 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1539  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 939 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1540  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.583s\n",
      "| Adam | epoch: 940 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1541  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 941 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1542  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.567s\n",
      "| Adam | epoch: 942 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1543  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.624s\n",
      "| Adam | epoch: 943 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1544  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.660s\n",
      "| Adam | epoch: 944 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1545  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 945 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1546  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 946 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1547  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.632s\n",
      "| Adam | epoch: 947 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1548  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.548s\n",
      "| Adam | epoch: 948 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04785 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1549  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.546s\n",
      "| Adam | epoch: 949 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1550  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 950 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1551  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 951 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1552  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.672s\n",
      "| Adam | epoch: 952 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1553  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.573s\n",
      "| Adam | epoch: 953 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04785 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1554  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 954 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1555  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.610s\n",
      "| Adam | epoch: 955 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1556  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.563s\n",
      "| Adam | epoch: 956 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1557  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.598s\n",
      "| Adam | epoch: 957 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1558  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.555s\n",
      "| Adam | epoch: 958 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1559  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.665s\n",
      "| Adam | epoch: 959 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1560  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.566s\n",
      "| Adam | epoch: 960 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1561  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.638s\n",
      "| Adam | epoch: 961 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1562  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.586s\n",
      "| Adam | epoch: 962 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1563  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 963 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1564  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 964 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1565  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 965 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1566  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 966 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1567  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 967 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1568  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.628s\n",
      "| Adam | epoch: 968 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.561s\n",
      "| Adam | epoch: 969 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04785 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 970 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1571  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.650s\n",
      "| Adam | epoch: 971 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1572  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.580s\n",
      "| Adam | epoch: 972 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1573  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.645s\n",
      "| Adam | epoch: 973 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1574  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.653s\n",
      "| Adam | epoch: 974 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1575  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.545s\n",
      "| Adam | epoch: 975 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1576  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 976 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1577  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.553s\n",
      "| Adam | epoch: 977 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1578  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.545s\n",
      "| Adam | epoch: 978 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1579  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 979 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1580  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.660s\n",
      "| Adam | epoch: 980 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1581  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.549s\n",
      "| Adam | epoch: 981 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1582  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.571s\n",
      "| Adam | epoch: 982 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1583  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.558s\n",
      "| Adam | epoch: 983 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1584  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 984 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1585  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.559s\n",
      "| Adam | epoch: 985 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1586  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.562s\n",
      "| Adam | epoch: 986 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1587  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.552s\n",
      "| Adam | epoch: 987 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1588  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.557s\n",
      "| Adam | epoch: 988 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04980 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1589  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.606s\n",
      "| Adam | epoch: 989 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1590  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.668s\n",
      "| Adam | epoch: 990 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1591  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.601s\n",
      "| Adam | epoch: 991 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1592  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.644s\n",
      "| Adam | epoch: 992 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1593  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.611s\n",
      "| Adam | epoch: 993 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1594  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.629s\n",
      "| Adam | epoch: 994 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1595  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.650s\n",
      "| Adam | epoch: 995 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1596  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.551s\n",
      "| Adam | epoch: 996 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1597  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.554s\n",
      "| Adam | epoch: 997 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1598  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.547s\n",
      "| Adam | epoch: 998 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1599  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.556s\n",
      "| Adam | epoch: 999 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n",
      "Training Step: 1600  | total loss: \u001b[1m\u001b[32m-9515.05273\u001b[0m\u001b[0m | time: 1.655s\n",
      "| Adam | epoch: 1000 | loss: -9515.05273 - acc: 1.0000 | val_loss: -9515.04883 - val_acc: 1.0000 -- iter: 3/3\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, n_epoch=1000, validation_set=0.1, show_metric=True,\n",
    "          snapshot_step=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:/home/lewis/wave_test/wave_default_lstm is not in all_model_checkpoint_paths. Manually adding it.\n",
      "WARNING:tensorflow:Error encountered when serializing layer_tensor/LSTM.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'list' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "model.save(\"wave_default_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = model.predict(X)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.71507150e-05,   6.58785066e-05,   4.97323017e-05, ...,\n",
       "         1.64314985e-11,   1.68006390e-11,   1.57788574e-11])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(test).reshape(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"test_wave.npy\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEICAYAAACavRnhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feba483b940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'Y_wave' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f0a1f1a92232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wave_data_org'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_wave\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_wave' is not defined"
     ]
    }
   ],
   "source": [
    "test = np.load(\"test_wave.npy\")\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('wave_data_predict')\n",
    "plt.plot(test)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('wave_data_org')\n",
    "plt.plot(Y_wave[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_boolean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-807b7b157892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_boolean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_boolean' is not defined"
     ]
    }
   ],
   "source": [
    "np.array(test_boolean).reshape(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_boolean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9fd5c4d75c67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_boolean.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_boolean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_boolean' is not defined"
     ]
    }
   ],
   "source": [
    "np.save(\"test_boolean.npy\", test_boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEICAYAAABmqDIrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7febc405e0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'Y_boolean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2d6f6482391c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wave_data_org'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_boolean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_boolean' is not defined"
     ]
    }
   ],
   "source": [
    "test_boolean = np.load(\"test_boolean.npy\")\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('wave_data_predict_boolean')\n",
    "plt.plot(test_boolean)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('wave_data_org')\n",
    "plt.plot(Y_boolean[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.46338452e-10   9.65994840e-10   1.09168419e-09 ...,   9.57508184e-10\n",
      "   9.40787226e-10   1.04098419e-09]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEICAYAAACavRnhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x127848ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEICAYAAACqMQjAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSFJKEiIikpSShIiIJPX/AaWaQgPePtG3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1114a70b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.title('wave_data_predict')\n",
    "plt.plot(test_boolean)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('wave_data_org')\n",
    "plt.plot(Y_wave[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wavfile.write(\"test.wav\", 44100, np.array(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.array(Y[0]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
